---
title: "Tuning ELM"
author: "Joshua Burkhart"
date: "March 6, 2016"
output: 
  pdf_document: 
    fig_width: 9
    fig_height: 6
    latex_engine: xelatex
---
# BMI 651: Tuning ELM

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(
  fig.path = "Figs/",
  message = FALSE,
  warning = FALSE,
  include = TRUE,
  echo = TRUE,
  error = TRUE,
  fig.width = 11,
  comment = NA
)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(reshape2)
library(infotheo)
library(stats)
library(car)
library(e1071)
library(bnlearn)
library(elmNN)
library(gbm)
set.seed(2013)
```

### Load Data

```{r}
setwd("~/SoftwareProjects/bmi_551_651_final/JoshBeta")

# gene id cols, cell line rows
expression <-
  read.table(
    "../data/expression.tsv",header = TRUE,sep = "\t",quote = "",row.names =
      1,check.names = FALSE,stringsAsFactors = FALSE
  )

# subtype cols, cell line rows
subtypes <-
  read.table(
    "../data/subtypes.txt",header = TRUE,sep = "\t",row.names = 1,check.names =
      TRUE
  )
subtypes[,1] <- subtypes[,1] %>% as.numeric()

# drug cols, cell line rows
training.classes <-
  t(
    read.table(
      "../data/training_set_answers.txt",header = TRUE,sep = "\t",check.names = FALSE
    )
  )
training.classes[,1:ncol(training.classes)] <-
  training.classes[,1:ncol(training.classes)] %>% as.numeric()

# Usage, id, drug, cell line columns, values in rows
scoring_and_test_set_id_mappings <-
  read.table(
    "../data/scoring_and_test_set_id_mappings.csv",header = TRUE,sep = ",",check.names =
      FALSE
  )

# organize subtypes, expression, train, test, classes...
sae <- rbind(data.frame(t(subtypes),check.names = FALSE),expression)

# split training data and kaggle holdout
training.features <- sae[,colnames(training.classes)]
kaggle.features <-
  sae[setdiff(colnames(sae),colnames(training.classes))]
```

### Scan for missing Data

```{r}
sum(is.na(training.features))
sum(is.na(kaggle.features))

sum(is.na(training.classes))
```

### Training and Kaggle Sets

```{r}
kaggle.classes <- data.frame()

#for (drug_idx in 1:nrow(training.classes))
#{
#  training.features_train <- training.features
#  training.classes_train <- training.classes[drug_idx,]
  
  ### Wilcoxan Rank Sum (Univariate Nonparametric Filter) for x dataset
#  print(c('Wilcoxan Rank Sum: ',drug_idx))
  
   x.P_LIM = .01
#  x.train_w_idx = vector()
#  x.train_p_val = vector()
#  x.all_features = vector()
  
#    for (i in 1:nrow(training.features_train)) {
#      x.w <-
#        wilcox.test(unlist(training.features_train[i,]) ~ training.classes_train, data =
#                      training.features_train)
#      if (x.w$p.value < x.P_LIM) {
#        x.train_w_idx <-append(x.train_w_idx,i)
#        x.train_p_val <- append(x.train_p_val,x.w$p.value)
#      }
#    }
#  x.all_features <- union(x.all_features,x.train_w_idx)
#}
  
x.all_features <- c(29,395,496,501,772,806,837,882,1159,1195,1200,1255,1315,1410,1467,1499,1529,1617,1836,1843,1858,1912,2133,2413,2698,2819,2829,2866,2923,3182,3198,3272,3334,3629,3773,4166,4179,4234,4321,4459,4516,4575,4664,4769,4914,5140,5231,5241,5269,5276,5425,5517,5599,5696,5746,5784,5802,5824,5865,5867,5870,5886,6067,6151,6226,6232,6289,6707,6723,6787,6982,7038,7081,7218,7269,7297,7322,7328,7425,7432,7695,8032,8196,8261,8316,8416,8538,8576,8616,8620,8660,8695,8709,8787,8840,8962,9128,9159,9458,9482,9564,9611,9643,9677,9712,9779,9816,9862,10159,10249,10270,10303,10881,10882,10924,10927,10934,10944,11104,11130,11252,11298,11651,11693,11872,11876,12089,12132,12154,12166,12199,12421,12430,12505,12593,12692,12722,12871,12933,13009,13214,13302,13428,13520,13610,13614,13646,13653,13833,13998,14016,14230,14479,14580,14627,14659,14678,14881,14897,15056,15097,15180,15584,15612,15835,16008,16010,16042,16170,16241,16388,16390,16405,16490,16555,16600,16623,16990,17008,17357,17374,17394,17531,17619,17803,17865,17931,18147,18327,18515,18542)

for (drug_idx in 1:nrow(training.classes))
{
  training.features_train <- training.features
  training.classes_train <- training.classes[drug_idx,]
  
  ### Cross Validation Loop
  
  # variables to store loop results
  x.cv_mhc <- NA
  x.cv_err <- Inf
  x.cv_elm <- NA
  
  for (cv_idx in seq(1:191)) {
    ### Split Training & Validation sets with random sampling (Monte Carlo cross validation)
    print(c(cv_idx,'Split Training & Validation sets'))
    
    # 5-fold Cross Validation
    index <-
      sample(1:ncol(training.features_train),round(0.7 * ncol(training.features_train)))
    training.features_train_cv <- training.features_train[,index]
    training.classes_train_cv <- training.classes_train[index]
    training.features_validation_cv <-
      training.features_train[,-index]
    training.classes_validation_cv <- training.classes_train[-index]
    
    feat_lim <- cv_idx
    elm_actv <- c("sig","sin","radbas","hardlim","hardlims","satlins","tansig","tribas","poslin","purelin")[mod(cv_idx%/%10 + 1,10) + 1]
    elm_nhid <- sample(c(10,15,30,60))[mod(cv_idx%/%4 + 1,4) + 1]
    
    if (x.all_features %>% length > 0)
    {
      #if(x.all_features %>% length > feat_lim){
      #  print(c("> 1000 features with p-values <",x.P_LIM,"... trimming to 1000..."))
      #  x.all_features <- x.all_features[order(as.double(x.train_p_val))[1:feat_lim]]
      #}
      
      x.all_features_cv <- sample(x.all_features,feat_lim)
      
      #now we can retain only our selected columns
      training.features_train_w <-
        training.features_train_cv[x.all_features_cv,]
      
      #add the class labels to the feature data frame
      training.train_elm_input <-
        data.frame(t(training.features_train_w),check.names = FALSE)
      
      #train model
      x.model <-
        elmtrain(x = training.train_elm_input,y = training.classes_train_cv,nhid = elm_nhid,actfun = elm_actv)
      
      ### Validation (on hold out data)
      print(c(cv_idx,'Validation'))
      
      #filter features
      training.validation_elm_input <-
        data.frame(t(training.features_validation_cv[x.all_features_cv,]), check.names = FALSE)
      
      #test
      x.class_ag <-
        table(round(predict(
          x.model,training.validation_elm_input,n.trees=tree_lim
        )[,1]),training.classes_validation_cv) %>%
        classAgreement()
      
      #misclassification rate
      x.err_rate <- 1 - x.class_ag$diag
      
      print(c(cv_idx,'cv error rate:',x.err_rate))
      
      if (x.err_rate < x.cv_err) {
        x.cv_mhc <- x.all_features_cv
        x.cv_err <- x.err_rate
        x.cv_elm <- x.model
        x.feat_lim <- feat_lim
        x.elm_nhid <- elm_nhid
        x.elm_actv <- elm_actv
      }
      
      if (x.err_rate == 0) {
        print("err_rate 0 reported, breaking out of loop...")
        break
      }
      
    }else{
      print(
        "Ignoring this cv loop because wilcoxon rank sum routine filtered all features from fold. Is the wilcoxon rank sum p-value set too low?"
      )
    }
  }
  
  ### Loop Results
  
  # markov blanket, & error rate
  print(c(x.cv_mhc,"best cv error rate:",x.cv_err))
  
  ### Predict Kaggle Holdout
  
  #filter features
  kaggle.elm_input <-
    data.frame(t(kaggle.features[x.cv_mhc,]), check.names = FALSE)
  
  #test
  kaggle.predictions <- predict(x.cv_elm,kaggle.elm_input,n.trees=tree_lim)
  
  kaggle.classes <- rbind(kaggle.classes,kaggle.predictions[,1])
}

rownames(kaggle.classes) <- rownames(training.classes)
colnames(kaggle.classes) <- colnames(kaggle.features)

kaggle.classes %>% str()
```

```{r}
mapping <- read.csv("../data/scoring_and_test_set_id_mappings.csv",check.names=FALSE)

kaggle.out <- data.frame()

for(row in 1:nrow(mapping))
{
  kaggle.out <- rbind(kaggle.out,c(mapping[row,"id"],kaggle.classes[mapping[row,"drug"],mapping[row,"cellline"]]))
}
colnames(kaggle.out)<-c('id','value')

fptr <- file(description="./tuningELM_out.csv",'w')
write.csv(kaggle.out,fptr,row.names=FALSE, quote = FALSE)
close(fptr)

print(c("err:",x.cv_err,"feat_lim:",x.feat_lim,"elm_nhid",x.elm_nhid,"actfun:",x.elm_actv))
```
