---
title: "Alpha Submission"
author: "Joshua Burkhart"
date: "March 3, 2016"
output: 
pdf_document: 
  fig_width: 9
  fig_height: 6
  latex_engine: xelatex
---
# BMI 651: Alpha Submission

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(fig.path = "Figs/",
                      message = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      echo = TRUE,
                      error = TRUE,
                      fig.width = 11,
                      comment = NA)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr 
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(reshape2)
library(infotheo)
library(stats)
library(ggbiplot)
library(car)
library(e1071)
library(bnlearn)
library(Rgraphviz)
set.seed(2013)
```

### Load Data

```{r}
setwd("/home/burkhart/Software/bmi_551_651_final/AlphaSubmission")

# gene id cols, cell line rows
expression <- t(read.table("../data/expression.tsv",header=TRUE,sep="\t",quote="",row.names=1,check.names=FALSE))

# subtype cols, cell line rows
subtypes <- read.table("../data/subtypes.txt",header=TRUE,sep="\t",row.names=1,check.names=FALSE)

# drug cols, cell line rows
training_set_answers <- read.table("../data/training_set_answers.txt",header=TRUE,sep="\t",check.names = FALSE)

# Usage, id, drug, cell line columns, values in rows
scoring_and_test_set_id_mappings <- read.table("../data/scoring_and_test_set_id_mappings.csv",header=TRUE,sep=",",check.names=FALSE)

# organize subtypes, expression, train, test, classes...
#sa <- merge(x=subtypes,y=training_set_answers,by="row.names",all=TRUE,row.names=1)
#rownames(sa) <- sa$Row.names
sae <- merge(x=subtypes,y=expression,by="row.names",all=TRUE,row.names=1)
rownames(sae) <- sae$Row.names

# split training data and kaggle holdout
training.features <- sae[rownames(training_set_answers),]
kaggle.features <- sae[setdiff(rownames(sae),rownames(training_set_answers)),]

# transform training answers to factors
training.classes <- lapply(training_set_answers[,colnames(training_set_answers)],factor)
```

### Scan for missing Data

```{r}
sum(is.na(training.features))
sum(is.na(kaggle.features))

sum(is.na(training.classes))
```

### Training & Test Sets (using first class only)

```{r}

### Split with random sampling

index <- sample(1:ncol(training.features),round(0.8*ncol(training.features)))
training.features_train <- training.features[,index]
training.classes_train <- training.classes[index,1]
training.features_test <- training.features[,-index]
training.classes_test <- training.classes[-index,1]
```

### Cross Validation Loop

```{r}
# variables to store loop results
x.cv_mhc <- NA
x.cv_err <- Inf
x.cv_svm <- NA

for(cv_idx in seq(1:3)){
  
  ### Split Training & Validation sets with random sampling (Monte Carlo cross validation)
  print(c(cv_idx,'Split Training & Validation sets'))
  
  # x
  index <-
    sample(1:ncol(training.features_train),round(0.9 * ncol(training.features_train)))
  training.features_train <- training.features_train[,index]
  training.classes_train <- training.classes_train[index]
  training.features_validation <- training.features_train[,-index]
  training.classes_validation <- training.classes_train[-index]
  
  ### Wilcoxan Rank Sum (Univariate Nonparametric Filter) for x dataset
  print(c(cv_idx,'Wilcoxan Rank Sum'))
  
  x.P_LIM = 1e-08
  x.train_w_idx = vector()
  
  #x
  for (i in 1:nrow(training.features_train)) {
    x.w <-
      wilcox.test(unlist(training.features_train[i,]) ~ training.classes_train,
                  data = training.features_train)
    if (x.w$p.value < x.P_LIM)
      x.train_w_idx <-
        append(x.train_w_idx,i) #store indices of significant features
  }
  
  #now we can retain only our selected columns
  training.features_train_w <-
    training.features_train[x.train_w_idx,]
  
  #add the class labels to the feature data frame
  training.net_input <-
    data.frame(t(training.features_train_w),check.names = FALSE)
  
  training.net_input["class"] <- training.classes_train
  
  ### Bayesian Net Construction (Multivariate Nonparametric Filter)
  print(c(cv_idx,'Bayesian Net Construction'))
  
  print(training.net_input %>% dim())
  
  training.hc <- hc(training.net_input)
  
  ### Markov Blanket Filter
  print(c(cv_idx,'Markov Blanket'))
  
  x.mhc <- mb(training.hc,node = "class")
  
  ### SVM
  print(c(cv_idx,'SVM'))
  
  #change to markov blanket features
  training.svm_input <-
    data.frame(t(training.features_train_w[x.mhc,]), check.names = FALSE)
  
  #train model
  x.model <-
    svm(training.svm_input,training.classes_train,kernel = "linear")
  
  ### Validation (on hold out data)
  print(c(cv_idx,'Validation'))
  
  #filter features
  training.svm_input <- data.frame(t(training.features_validation[x.mhc,]), check.names = FALSE)
  
  #test
  x.class_ag <-
    table(predict(x.model,training.svm_input),training.classes_validation) %>%
    classAgreement()
  
  #misclassification rate
  x.err_rate <- 1 - x.class_ag$diag
  
  if(x.err_rate < x.cv_err){
    x.cv_mhc <- x.mhc
    x.cv_err <- x.err_rate
    x.cv_svm <- x.model
  }
}
```

### Loop Results

```{r}
# x feat. similarity with lm filter, markov blanket, & error rate
print(c(x.cv_mhc,x.cv_err))
```

### Test

```{r}
  #filter features
  training.svm_input <- data.frame(t(training.features_test[x.cv_mhc,]), check.names = FALSE)
  
  #test
  x.class_ag <-
    table(predict(x.cv_svm,training.svm_input),training.classes_test) %>%
    classAgreement()
  
  #misclassification rate
  x.validation_err_rate <- 1 - x.class_ag$diag

  print(x.validation_err_rate)
```
