---
title: "Alpha Submission"
author: "Joshua Burkhart"
date: "March 3, 2016"
output: 
pdf_document: 
  fig_width: 9
  fig_height: 6
  latex_engine: xelatex
---
# BMI 651: Alpha Submission

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(fig.path = "Figs/",
                      message = FALSE,
                      warning = FALSE,
                      include = TRUE,
                      echo = TRUE,
                      error = TRUE,
                      fig.width = 11,
                      comment = NA)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr 
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(reshape2)
library(infotheo)
library(stats)
library(ggbiplot)
library(car)
library(e1071)
library(bnlearn)
library(Rgraphviz)
set.seed(2013)
```

### Load Data

```{r}
setwd("/home/burkhart/Software/bmi_551_651_final/AlphaSubmission")
expression <- read.table("../data/expression.tsv",header=TRUE,sep="\t",quote="",row.names=1)

# gene id rows, cell type columns

subtypes <- read.table("../data/subtypes.txt",header=TRUE,sep="\t",row.names=1)

# cell type rows, subtype column

training_set_answers <- read.table("../data/training_set_answers.txt",header=TRUE,sep="\t")

# cell type rows, drug columns

scoring_and_test_set_id_mappings <- read.table("../data/scoring_and_test_set_id_mappings.csv",header=TRUE,sep=",")

# Usage, id, drug, cell line columns, values in rows

```

### Scan for missing Data

```{r}
sum(is.na(hw3B.x.genotypes))
sum(is.na(hw3B.x.probe_intensities))
sum(is.na(hw3B.xq.cell_type))
sum(is.na(hw3B.xq.gene_expressions))

sum(is.na(hw3B.x.classes))
sum(is.na(hw3B.xq.classes))
```

```{r}
t(hw3B.xq.gene_expressions) %>% summary()
hw3B.xq.gene_expressions <- na.omit(hw3B.xq.gene_expressions)
```

### Transform positive (E3.25) and negative (E3.5 and E4.5) values 1 and 0, respectively

```{r}
hw3B.x.classes <- as.character(hw3B.x.classes)
hw3B.x.classes[hw3B.x.classes == "E3.25"] = 1
hw3B.x.classes[hw3B.x.classes == "E3.5"] = 0
hw3B.x.classes[hw3B.x.classes == "E4.5"] = 0
hw3B.x.classes <- as.factor(hw3B.x.classes)

hw3B.xq.classes <- as.character(hw3B.xq.classes)
hw3B.xq.classes[hw3B.xq.classes == "E3.25"] = 1
hw3B.xq.classes[hw3B.xq.classes == "E3.5"] = 0
hw3B.xq.classes[hw3B.xq.classes == "E4.5"] = 0
hw3B.xq.classes <- as.factor(hw3B.xq.classes)
```

### Combine Features for x and xq datasets

```{r}
hw3B.x.features <- rbind(as.numeric(as.factor(hw3B.x.genotypes)),hw3B.x.probe_intensities)
hw3B.xq.features <- rbind(as.numeric(as.factor(hw3B.xq.cell_type)),hw3B.xq.gene_expressions)
```

### Training & Test Sets

```{r}

### Split with random sampling

# x
index <- sample(1:ncol(hw3B.x.features),round(0.8*ncol(hw3B.x.features)))
hw3B.x.features_train <- hw3B.x.features[,index]
hw3B.x.classes_train <- hw3B.x.classes[index]
hw3B.x.features_test <- hw3B.x.features[,-index]
hw3B.x.classes_test <- hw3B.x.classes[-index]

# xq
index <- sample(1:ncol(hw3B.xq.features),round(0.8*ncol(hw3B.xq.features)))
hw3B.xq.features_train <- hw3B.xq.features[,index]
hw3B.xq.classes_train <- hw3B.xq.classes[index]
hw3B.xq.features_test <- hw3B.xq.features[,-index]
hw3B.xq.classes_test <- hw3B.xq.classes[-index]
```

### Cross Validation Loop

```{r}
# variables to store loop results
x.cv_feat_sd <- NA
x.cv_feat_mean <- NA
x.cv_pct_lm <- NA
x.cv_mhc <- NA
x.cv_err <- Inf
x.cv_svm <- NA

xq.cv_feat_sd <- NA
xq.cv_feat_mean <- NA
xq.cv_pct_lm <- NA
xq.cv_mhc <- NA
xq.cv_err <- Inf
xq.cv_svm <- NA

for(cv_idx in seq(1:3)){
  
  ### Split Training & Validation sets with random sampling (Monte Carlo cross validation)
  print(c(cv_idx,'Split Training & Validation sets'))
  
  # x
  index <-
    sample(1:ncol(hw3B.x.features_train),round(0.9 * ncol(hw3B.x.features_train)))
  hw3B.x.features_train <- hw3B.x.features_train[,index]
  hw3B.x.classes_train <- hw3B.x.classes_train[index]
  hw3B.x.features_validation <- hw3B.x.features_train[,-index]
  hw3B.x.classes_validation <- hw3B.x.classes_train[-index]
  
  # xq
  index <-
    sample(1:ncol(hw3B.xq.features_train),round(0.9 * ncol(hw3B.xq.features_train)))
  hw3B.xq.features_train <- hw3B.xq.features_train[,index]
  hw3B.xq.classes_train <- hw3B.xq.classes_train[index]
  hw3B.xq.features_validation <- hw3B.xq.features_train[,-index]
  hw3B.xq.classes_validation <- hw3B.xq.classes_train[-index]
  
  ### Z score training set
  print(c(cv_idx,'Z score training set'))

  # takes too long...
  if(FALSE){
    # x
    x.train_sd = matrix()
    x.train_mean = matrix()
    hw3B.x.features_train_normalized <- hw3B.x.features_train
    for (i in 1:nrow(hw3B.x.features_train)){
      # each row is a feature, first is a factor (genotype)
      x.train_sd[i] = sd(hw3B.x.features_train[i,])
      x.train_mean[i] = apply(hw3B.x.features_train[i,],1,mean)
      for (j in 1:ncol(hw3B.x.features_train)){
        # each column is a sample
        hw3B.x.features_train_normalized[i,j] =
          (hw3B.x.features_train[i,j] - x.train_mean[i]) / x.train_sd[i]
      }
    }
  }
  
  # xq
  xq.train_sd = matrix()
  xq.train_mean = matrix()
  hw3B.xq.features_train_normalized <- hw3B.xq.features_train
  for (i in 1:nrow(hw3B.xq.features_train)){
    # each row is a feature, first is a factor (cell_type)
    xq.train_sd[i] = sd(hw3B.xq.features_train[i,])
    xq.train_mean[i] = apply(hw3B.xq.features_train[i,],1,mean)
    for (j in 1:ncol(hw3B.xq.features_train)){
      # each column is a sample
      hw3B.xq.features_train_normalized[i,j] =
        (hw3B.xq.features_train[i,j] - xq.train_mean[i]) / xq.train_sd[i]
    }
  }
  
  ### Wilcoxan Rank Sum (Univariate Nonparametric Filter) for x dataset
  print(c(cv_idx,'Wilcoxan Rank Sum'))
  
  x.P_LIM = 1e-08
  x.train_w_idx = vector()
  
  #x
  for (i in 1:nrow(hw3B.x.features_train)) {
    x.w <-
      wilcox.test(unlist(hw3B.x.features_train[i,]) ~ hw3B.x.classes_train,
                  data = hw3B.x.features_train)
    if (x.w$p.value < x.P_LIM)
      x.train_w_idx <-
        append(x.train_w_idx,i) #store indices of significant features
    #print(x.train_w_idx %>% length()) debugging
  }
  #print(x.train_w_idx %>% length())
  
  #now we can retain only our selected columns
  hw3B.x.features_train_w <-
    hw3B.x.features_train[x.train_w_idx,]
  
  #add the class labels to the feature data frame
  hw3B.xq.net_input <-
    data.frame(t(hw3B.xq.features_train_normalized), check.names = FALSE)
  hw3B.x.net_input <-
    data.frame(t(hw3B.x.features_train_w),check.names = FALSE)
  
  hw3B.xq.net_input["class"] <- hw3B.xq.classes_train
  hw3B.x.net_input["class"] <- hw3B.x.classes_train
  
  ### Linear Fit (Univariate Parametric Filter) for x dataset
  print(c(cv_idx,'Linear Fit'))
  
  x.fit <- lmFit(hw3B.x.features_train,as.numeric(hw3B.x.classes_train))
  x.fit <- eBayes(x.fit)
  topTable(x.fit,coef = 1)
  
  x.limmaRes = topTable(x.fit,coef = 1,  p.value = x.P_LIM, number = 500)
  x.ilen <-
    intersect(x.limmaRes %>% rownames(),hw3B.x.net_input %>% colnames()) %>% length()
  
  ### Bayesian Net Construction (Multivariate Nonparametric Filter)
  print(c(cv_idx,'Bayesian Net Construction'))
  
  print(hw3B.x.net_input %>% dim())
  
  bn.xq.hc <- hc(hw3B.xq.net_input)
  #bn.xq.fast.iamb <- fast.iamb(hw3B.xq.net_input)
  bn.x.hc <- hc(hw3B.x.net_input)
  # takes too long
  #bn.x.fast.iamb <- fast.iamb(hw3B.x.net_input)
  
  # show bayesian net for debugging
  #par(mfrow = c(1,2))
  #graphviz.plot(bn.xq.hc)
  #graphviz.plot(bn.xq.fast.iamb)
  
  ### Markov Blanket Filter
  print(c(cv_idx,'Markov Blanket'))
  
  # fast.iamb takes too long
  xq.mhc <- mb(bn.xq.hc,node = "class")
  #xq.mfi <- mb(bn.xq.fast.iamb,node="class")
  x.mhc <- mb(bn.x.hc,node = "class")
  #x.mfi <- mb(bn.x.fast.iamb,node="class")
  
  ### SVM
  print(c(cv_idx,'SVM'))
  
  #change to markov blanket features
  hw3B.x.svm_input <-
    data.frame(t(hw3B.x.features_train_w[x.mhc,]), check.names = FALSE)
  
  hw3B.xq.svm_input <-
    data.frame(t(hw3B.xq.features_train_normalized[xq.mhc,]), check.names = FALSE)
  
  #train model
  x.model <-
    svm(hw3B.x.svm_input,hw3B.x.classes_train,kernel = "linear")
  
  xq.model <-
    svm(hw3B.xq.svm_input,hw3B.xq.classes_train,kernel = "linear")
  
  ### Validation (on hold out data)
  print(c(cv_idx,'Validation'))
  
  #Z score validation data using previously calculated means & standard deviations
  
  # x
  if(FALSE){
    hw3B.x.features_validation_normalized <- hw3B.x.features_validation
    for (i in 1:nrow(hw3B.x.features_validation)){
      # each row is a feature, first is a factor (genotype)
      for (j in 1:ncol(hw3B.x.features_validation)){
        # each column is a sample
        hw3B.x.features_validation_normalized[i,j] =
          (hw3B.x.features_validation[i,j] - x.train_mean[i]) / x.train_sd[i]
      }
    }
  }
  
  # xq
  hw3B.xq.features_validation_normalized <- hw3B.xq.features_validation
  for (i in 1:nrow(hw3B.xq.features_validation)){
    # each row is a feature, first is a factor (cell_type)
    for (j in 1:ncol(hw3B.xq.features_validation)){
      # each column is a sample
      hw3B.xq.features_validation_normalized[i,j] =
        (hw3B.xq.features_validation[i,j] - xq.train_mean[i]) / xq.train_sd[i]
    }
  }
  
  #filter features
  hw3B.x.svm_input <- data.frame(t(hw3B.x.features_validation[x.mhc,]), check.names = FALSE)
  hw3B.xq.svm_input <- data.frame(t(hw3B.xq.features_validation_normalized[xq.mhc,]),check.names = FALSE)
  
  #test
  x.class_ag <-
    table(predict(x.model,hw3B.x.svm_input),hw3B.x.classes_validation) %>%
    classAgreement()
  
  xq.class_ag <-
    table(predict(xq.model,hw3B.xq.svm_input),hw3B.xq.classes_validation) %>%
    classAgreement()
  
  #misclassification rate
  x.err_rate <- 1 - x.class_ag$diag
  xq.err_rate <- 1 - xq.class_ag$diag
  
  if(x.err_rate < x.cv_err){
    x.cv_feat_sd <- NA #x.train_sd
    x.cv_feat_mean <- NA #x.train_mean
    x.cv_pct_lm <- x.ilen / nrow(hw3B.x.net_input)
    x.cv_mhc <- x.mhc
    x.cv_err <- x.err_rate
    x.cv_svm <- x.model
  }
  
  if(xq.err_rate < xq.cv_err){
    xq.cv_feat_sd <- xq.train_sd
    xq.cv_feat_mean <- xq.train_mean
    xq.cv_pct_lm <- NA
    xq.cv_mhc <- xq.mhc
    xq.cv_err <- xq.err_rate
    xq.cv_svm <- xq.model
  }
}
```

### Loop Results

```{r}

# x feat. similarity with lm filter, markov blanket, & error rate
print(c(x.cv_pct_lm,x.cv_mhc,x.cv_err))

# xq markov blanket & error rate
print(c(xq.cv_mhc,xq.cv_err))
```

### Test

```{r}

  # x
  if(FALSE){
    hw3B.x.features_test_normalized <- hw3B.x.features_test
    for (i in 1:nrow(hw3B.x.features_test)){
      # each row is a feature, first is a factor (genotype)
      for (j in 1:ncol(hw3B.x.features_test)){
        # each column is a sample
        hw3B.x.features_test_normalized[i,j] =
          (hw3B.x.features_test[i,j] - x.cv_feat_mean[i]) / x.cv_feat_sd[i]
      }
    }
  }
  
  # xq
  hw3B.xq.features_test_normalized <- hw3B.xq.features_test
  for (i in 1:nrow(hw3B.xq.features_test)){
    # each row is a feature, first is a factor (cell_type)
    for (j in 1:ncol(hw3B.xq.features_test)){
      # each column is a sample
      hw3B.xq.features_test_normalized[i,j] =
        (hw3B.xq.features_test[i,j] - xq.cv_feat_mean[i]) / xq.cv_feat_sd[i]
    }
  }
  
  #filter features
  hw3B.x.svm_input <- data.frame(t(hw3B.x.features_test[x.cv_mhc,]), check.names = FALSE)
  hw3B.xq.svm_input <- data.frame(t(hw3B.xq.features_test_normalized[xq.cv_mhc,]),check.names = FALSE)
  
  #test
  x.class_ag <-
    table(predict(x.cv_svm,hw3B.x.svm_input),hw3B.x.classes_test) %>%
    classAgreement()
  
  xq.class_ag <-
    table(predict(xq.cv_svm,hw3B.xq.svm_input),hw3B.xq.classes_test) %>%
    classAgreement()
  
  #misclassification rate
  x.validation_err_rate <- 1 - x.class_ag$diag
  xq.validation_err_rate <- 1 - xq.class_ag$diag

  print(x.validation_err_rate)
  print(xq.validation_err_rate)
```
