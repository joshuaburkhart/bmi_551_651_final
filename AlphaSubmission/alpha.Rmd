---
title: "Alpha Submission"
author: "Joshua Burkhart"
date: "March 3, 2016"
output: 
pdf_document: 
fig_width: 9
fig_height: 6
latex_engine: xelatex
---
# BMI 651: Alpha Submission

```{r global_options, echo=FALSE, include=FALSE, error=FALSE}
knitr::opts_chunk$set(
  fig.path = "Figs/",
  message = FALSE,
  warning = FALSE,
  include = TRUE,
  echo = TRUE,
  error = TRUE,
  fig.width = 11,
  comment = NA
)
```

```{r, echo=FALSE, include=FALSE}
library(MASS)
library(plyr) #this must be loaded before dplyr
library(dplyr)
library(ggplot2)
library(broom)
library(knitr)
library(magrittr)
library(reshape2)
library(infotheo)
library(stats)
library(car)
library(e1071)
library(bnlearn)
set.seed(2013)
```

### Load Data

```{r}
setwd("/Users/joshuaburkhart/SoftwareProjects/bmi_551_651_final/AlphaSubmission")

# gene id cols, cell line rows
expression <-
  read.table(
    "../data/expression.tsv",header = TRUE,sep = "\t",quote = "",row.names =
      1,check.names = FALSE,stringsAsFactors = FALSE
  )

# subtype cols, cell line rows
subtypes <-
  read.table(
    "../data/subtypes.txt",header = TRUE,sep = "\t",row.names = 1,check.names =
      TRUE
  )
subtypes[,1] <- subtypes[,1] %>% as.numeric()

# drug cols, cell line rows
training.classes <-
  t(
    read.table(
      "../data/training_set_answers.txt",header = TRUE,sep = "\t",check.names = FALSE
    )
  )
training.classes[,1:ncol(training.classes)] <-
  training.classes[,1:ncol(training.classes)] %>% as.numeric()

# Usage, id, drug, cell line columns, values in rows
scoring_and_test_set_id_mappings <-
  read.table(
    "../data/scoring_and_test_set_id_mappings.csv",header = TRUE,sep = ",",check.names =
      FALSE
  )

# organize subtypes, expression, train, test, classes...
sae <- rbind(data.frame(t(subtypes),check.names = FALSE),expression)

# split training data and kaggle holdout
training.features <- sae[,colnames(training.classes)]
kaggle.features <-
  sae[setdiff(colnames(sae),colnames(training.classes))]
```

### Scan for missing Data

```{r}
sum(is.na(training.features))
sum(is.na(kaggle.features))

sum(is.na(training.classes))
```

### Training & Test Sets (using first class only)

```{r}
### Split with random sampling

index <-
  sample(1:ncol(training.features),round(0.7 * ncol(training.features)))
training.features_train <- training.features[,index]
training.classes_train <- training.classes[3,index]
training.features_test <- training.features[,-index]
training.classes_test <- training.classes[3,-index]
```

### Cross Validation Loop

```{r}
# variables to store loop results
x.cv_mhc <- NA
x.cv_err <- Inf
x.cv_svm <- NA

for (cv_idx in seq(1:3)) {
  ### Split Training & Validation sets with random sampling (Monte Carlo cross validation)
  print(c(cv_idx,'Split Training & Validation sets'))
  
  # x
  index <-
    sample(1:ncol(training.features_train),round(0.7 * ncol(training.features_train)))
  training.features_train_cv <- training.features_train[,index]
  training.classes_train_cv <- training.classes_train[index]
  training.features_validation_cv <-
    training.features_train[,-index]
  training.classes_validation_cv <- training.classes_train[-index]
  
  ### Wilcoxan Rank Sum (Univariate Nonparametric Filter) for x dataset
  print(c(cv_idx,'Wilcoxan Rank Sum'))
  
  x.P_LIM = .005
  x.train_w_idx = vector()
  
  # only use for numeric expression data
  for (i in 1:nrow(training.features_train_cv)) {
    x.w <-
      wilcox.test(unlist(training.features_train_cv[i,]) ~ training.classes_train_cv, data=training.features_train_cv)
    if (x.w$p.value < x.P_LIM) {
      x.train_w_idx <-
        append(x.train_w_idx,i) #store indices of significant features
      #print(c(x.w$p.value,"sufficient p-value for",i))
    }
  }
  
  print("wilcox complete.")
  
  #now we can retain only our selected columns
  training.features_train_w <-
    training.features_train_cv[c(x.train_w_idx,1),] # adding row 1 (subtype)
  
  #add the class labels to the feature data frame
  training.net_input <-
    data.frame(t(training.features_train_w),check.names = FALSE)
  
  training.net_input["class"] <- training.classes_train_cv
  
  ### Bayesian Net Construction (Multivariate Nonparametric Filter)
  print(c(cv_idx,'Bayesian Net Construction'))
  
  print(training.net_input %>% dim())
  
  training.hc <- hc(training.net_input)
  
  ### Markov Blanket Filter
  print(c(cv_idx,'Markov Blanket'))
  
  x.mhc <- mb(training.hc,node = "class")
  
  ### SVM
  print(c(cv_idx,'SVM'))
  
  #change to markov blanket features
  training.train_svm_input <-
    data.frame(t(training.features_train_w[x.mhc,]), check.names = FALSE)
  
  #train model
  x.model <-
    svm(x=training.train_svm_input,y=training.classes_train_cv)
  
  ### Validation (on hold out data)
  print(c(cv_idx,'Validation'))
  
  #filter features
  training.validation_svm_input <-
    data.frame(t(training.features_validation_cv[x.mhc,]), check.names = FALSE)
  
  #test
  x.class_ag <-
    table(round(predict(x.model,training.validation_svm_input)),training.classes_validation_cv) %>%
    classAgreement()
  
  #misclassification rate
  x.err_rate <- 1 - x.class_ag$diag
  
  if (x.err_rate < x.cv_err) {
    x.cv_mhc <- x.mhc
    x.cv_err <- x.err_rate
    x.cv_svm <- x.model
  }
}
```

### Loop Results

```{r}
# markov blanket, & error rate
print(c(x.cv_mhc,x.cv_err))
```

### Test

```{r}
#filter features
training.svm_input <-
  data.frame(t(training.features_test[x.cv_mhc,]), check.names = FALSE)

#test
x.class_ag <-
  table(round(predict(x.cv_svm,training.svm_input)),training.classes_test) %>%
  classAgreement()

#misclassification rate
x.validation_err_rate <- 1 - x.class_ag$diag

print(x.validation_err_rate)
```
